一、hive的介绍及其发展
"27.38.5.159" "-" "31/Aug/2015:00:04:37 +0800" "GET /course/view.php?id=27 HTTP/1.1" "303" "440" - "http://www.micro.com/user.php?act=mycourse" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/31.0.1650.63 Safari/537.36" "-" "learn.micro.com"

-》需求：每小时的PV数统计
	-》数据采集
		-》将日志上传到HDFS
	-》数据清洗
		-》功能
			-》数据字段的提取
				"27.38.5.159"	"31/Aug/2015:00:04:37 +0800"  /course/view.php
			-》数据字段的补充（一般存储在RDBMS）
				-》用户的信息，订单的信息
			-》字段格式化
				-》对字段去除双引号
				-》31/Aug/2015:00:04:37 +0800  -》 2015-08-31 00::04:37  20150831 00:04:37 
	-》数据分析（MapReduce）
		-》input
		-》map
			key：小时
			value：URL
		-》shuffle
			key,{url1,url2……}
		-》reduce
			key,count(url)
		-》output

	
	-》如果使用SQL实现
		-》创建需要分析的表
			hpsk_log{
			ip,
			time,
			url
			……
			}
		-》将小时提取出来
			time：20150831000437
			select ip,substring(time,9,2) hour,url from hpsk_log;
		-》进行每小时的PV统计
			预期结果：
				hour	pv
				00		100
				01		200
			select hour ,count(url) pv from 
			(select ip,substring(time,9,2) hour,url from hpsk_log)
			group by hour;
	
	-》hive分析的本质：
		-》将数据文件映射成表，使用SQL进行分析
	-》使用MapReduce分析的问题
		-》模式固定
		-》没有schema，缺少类似于SQL的查询语言
		-》开发成本比较高
	-》SQL on HADOOP：为Hadoop分析处理提供SQL语言的框架
		-》hive
		-》presto:facebook开源，内存式的处理，京东用的比较多
		-》impala:基于内存式的SQL处理框架
		-》spark SQL
	-》hive的特点：数据仓库
		-》建立在Hadoop之上
		-》处理结构化的数据
		-》存储依赖于HDSF：hive表中的数据是存储在hdfs之上
		-》SQL语句的执行依赖于MapReduce
		-》hive的功能：让Hadoop实现了SQL的接口，实际就是将SQL语句转化为MapReduce程序
		-》hive的本质就是Hadoop的客户端
		
		
二、hive的安装部署及其架构
	-》官网：http://hive.apache.org/
	-》常用版本
		-》0.13.1：提供更多的SQL接口，稳定性更高
		-》1.2.1：提供更多的SQL接口，在处理性能方面的优化
	-》安装：
		-》下载解压
			tar -zxvf apache-hive-0.13.1-bin.tar.gz -C /opt/modules/
		-》修改配置文件
			-》修改hive-env.sh
				mv hive-env.sh.template hive-env.sh
				HADOOP_HOME=/opt/modules/hadoop-2.5.0
				export HIVE_CONF_DIR=/opt/modules/hive-0.13.1-bin/conf
			-》环境变量的功能
				-》用于全局访问
				-》用于框架集成时的访问
		-》创建数据仓库目录
			  bin/hdfs dfs -mkdir       /tmp
			  bin/hdfs dfs -mkdir   -p  /user/hive/warehouse
			  bin/hdfs dfs -chmod g+w   /tmp
			  bin/hdfs dfs -chmod g+w   /user/hive/warehouse
		-》启动hive客户端

	-》hive的架构
		-》metastore
			-》功能：用于存储hive中数据库、表、与数据文件的映射
			-》存储路径：数据库
				-》默认存储在自带的Derby数据库中：metastore_db
				-》在企业中一般会修改为MySQL或者oracle数据库
		-》client
			-》客户端
			-》驱动
			-》SQL解析器
			-》语句优化器
			-》物理计划
			-》执行
		-》Hadoop
			-》HDFS：用于存储hive中表的数据
				默认的hive的存储路径：/user/hive/warehouse
			-》MapReduce
				用于hive分析计算，将SQL进行解析并处理
		-》hive支持的计算框架
			-》MapReduce
			-》Tez
			-》spark
			
三、配置MySQL存储metastore
	-》默认是Derby数据库存储
		-》缺点：Derby存储使用文件存储，同一时间只能启动一个数据库实例
		-》安全性不高
	-》配置使用MySQL
		-》安装MySQL
			-》检查是否已安装MySQL
				sudo rpm -aq |grep mysql
			-》安装
				sudo yum install -y mysql-server
			-》启动MySQL的服务
				sudo service mysqld start 
			-》配置开机启动
				sudo chkconfig mysqld on
			-》配置管理员密码
				mysqladmin -u root password '123456'
			-》进入MySQL
				mysql -u root -p
		-》配置用户访问权限
			-》查看
				select User,Host,Password  from user; 
			-》进行授权
				grant all privileges on *.* to 'root'@'%' identified by '123456' with grant option;
			-》将其他的用户权限删除
				delete from user where host='127.0.0.1';
				delete from user where host='localhost'; 
				delete from user where host='bigdata-training01.hpsk.com'; 
			-》刷新权限
				flush privileges;
			-》重启MySQL
				sudo service mysqld restart
	-》配置hive使用MySQL存储元数据
		-》创建hive-site文件
		-》编辑，写入配置项
			<property>
			  <name>javax.jdo.option.ConnectionURL</name>
			  <value>jdbc:mysql://bigdata-training01.hpsk.com:3306/metastore?createDatabaseIfNotExist=true</value>
			  <description>JDBC connect string for a JDBC metastore</description>
			</property>
			<property>
			  <name>javax.jdo.option.ConnectionDriverName</name>
			  <value>com.mysql.jdbc.Driver</value>
			  <description>Driver class name for a JDBC metastore</description>
			</property>
			<property>
			  <name>javax.jdo.option.ConnectionUserName</name>
			  <value>root</value>
			  <description>username to use against metastore database</description>
			</property>
			<property>
			  <name>javax.jdo.option.ConnectionPassword</name>
			  <value>123456</value>
			  <description>password to use against metastore database</description>
			</property>
		-》将连接驱动放入hive的lib目录下
			cp /opt/tools/mysql-connector-java-5.1.27-bin.jar lib/
		-》启动hive
		
四、基本命令和常用属性
	-》基本命令
		-》数据库
			create database if not exists student;
			show databases;
			use student;
		-》表
			-》将日志文件映射成表
			-》表的结构要与日志文件的结构一致
create table if not exists stu_tmp(
number string,
name string
) row format delimited fields terminated by '\t';
			-》加载数据
				load data local inpath '/opt/modules/hive-0.13.1-bin/student.txt' into table stu_info;
			-》查看表
				show tables;
		-》函数
			-》show functions;
			-》desc function substring;
			-》desc function extended substring;
	-》hive的原理
		-》所有检索查询运行的是MapReduce
		-》hive中的数据库、表在hdfs上都是一个文件目录
	-》常用属性的配置
		-》数据仓库目录的配置
			-》默认值：/user/hive/warehouse
				<property>
				  <name>hive.metastore.warehouse.dir</name>
				  <value>/user/hive/warehouse</value>
				  <description>location of default database for the warehouse</description>
				</property>
			-》default数据库默认的存储目录：/user/hive/warehouse
		-》启用自定义的日志配置信息
			-》将conf目录中的hive-log4j.properties.template重命名
				mv hive-log4j.properties.template hive-log4j.properties
			-》修改日志存储路径
				hive.log.threshold=ALL
				hive.root.logger=INFO,DRFA
				hive.log.dir=/opt/modules/hive-0.13.1-bin/logs
				hive.log.file=hive.log
		-》显示当前当前数据库
			<property>
			  <name>hive.cli.print.current.db</name>
			  <value>true</value>
			  <description>Whether to include the current database in the Hive prompt.</description>
			</property>
		-》显示表头信息
			<property>
			  <name>hive.cli.print.header</name>
			  <value>true</value>
			  <description>Whether to print the names of the columns in query output.</description>
			</property>
	-》启动hive时，可以查看的帮助命令
		-》bin/hive -help
			usage: hive
			 -d,--define <key=value>          Variable subsitution to apply to hive
											  commands. e.g. -d A=B or --define A=B
				--database <databasename>     Specify the database to use
			 -e <quoted-query-string>         SQL from command line
			 -f <filename>                    SQL from files
			 -H,--help                        Print help information
			 -h <hostname>                    connecting to Hive Server on remote host
				--hiveconf <property=value>   Use value for given property
				--hivevar <key=value>         Variable subsitution to apply to hive
											  commands. e.g. --hivevar A=B
			 -i <filename>                    Initialization SQL file
			 -p <port>                        connecting to Hive Server on port number
			 -S,--silent                      Silent mode in interactive shell
			 -v,--verbose                     Verbose mode (echo executed SQL to the
											  console)
		-》启动时指定进入某个数据库
			bin/hive --database student
		-》在命令行执行SQL语句:单条SQL语句
			bin/hive -e "show databases"
			bin/hive -e "show databases" >> /opt/datas/hive.exec.log
		-》在命令行执行一个包含SQL语句的文件：多条sql语句
			bin/hive -f /opt/datas/hivetest.sql >> /opt/datas/hive.exec.log
		-》运行hive时，传递变量参数（可以是自定义，也可以是配置变量）
			bin/hive --hiveconf hive.cli.print.header=false
	-》常用的交互式操作
		-》退出
			exit、quit
		-》set命令
			-》查看某个属性的值
				set hive.cli.print.header;
				hive.cli.print.header=false
			-》修改某个属性的值，并且立即生效
				set hive.cli.print.header=true;
		-》!用于在hive shell中执行Linux命令
			!ls /;
		-》dfs命令用于在hive shell中执行hdfs的命令
			dfs -ls /;
			dfs -ls /user/hive/warehouse;
			
五、数据库与表的管理
	-》数据库
		-》创建
create table if not exists tmp3_table(
number string,
name string
) row format delimited fields terminated by '\t';
load data local inpath '/opt/modules/hive-0.13.1-bin/student.txt' into table tmp3_table;
			-》本地导入，将本地文件复制到了hdfs上表的目录下
			-》hdfs导入，直接将文件移动到了表的目录下
			-》第一种
				create database if not exists tmp1;
			-》第二种：[LOCATION hdfs_path]，指定数据库在hdfs上的目录
				create database if not exists tmp2 location '/hive/tmp2';
		-》删除
			drop database tmp1;
			删除非空数据库：
			drop database tmp1 cascade;
			-》删除时会删除元数据及HDFS的目录
		-》查看信息
			desc database EXTENDED tmp2;
	-》表
		-》创建
CREATE  [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
  [(col_name data_type [COMMENT col_comment], ... [constraint_specification])]
  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]               
   [ROW FORMAT row_format] 
   [STORED AS file_format]
  [LOCATION hdfs_path]
  [AS select_statement];
  
		-》第一种：普通方式
		create table if not exists tmp3(
		col1 type,
		col2 type……	
		)
		row format delimited fields terminated by '\t'
		stored as textfile
		location 'hdfs_path';

		-》第二种：as：子查询方式
		create table tmp3_as as select name from tmp3_table;
		-》第三种：like
		CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
		  LIKE existing_table_or_view_name
  [LOCATION hdfs_path];
		create table tmp3_like like tmp3_table;
		-》as与like的区别
			-》as：将子查询的结果，包括数据和表结构放入的新的表中
			-》like:只是复制了表结构
		-》删除
			drop table tmp3_like;
			同时删除元数据和hdfs的存储目录
		-》清空
			TRUNCATE table tmp3_as;
		-》描述
			desc tmp3_table;
			desc extended tmp3_table;
			desc formatted tmp3_table;
	-》创建员工表与部门表
		create database emp_test;
create table emp(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int
)
row format delimited fields terminated by '\t';
load data local inpath '/opt/datas/emp.txt' into table emp;

create table dept(
deptno int,
dname string,
loc string
)
row format delimited fields terminated by '\t';
load data local inpath '/opt/datas/dept.txt' overwrite into table dept;

-》hive中表的类型
	-》管理表：
		Table Type:             MANAGED_TABLE  

		
create table emp_m(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int
)
row format delimited fields terminated by '\t';
load data local inpath '/opt/datas/emp.txt' into table emp_m;

drop table emp_m;

	-》外部表：EXTERNAL
create EXTERNAL table emp_e(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm double,
deptno int
)
row format delimited fields terminated by '\t';
load data local inpath '/opt/datas/emp.txt' into table emp_e;

drop table emp_e;	

	-》对比
		-》管理表：
			-》删除时会删除元数据
			-》删除时会删除HDFS的数据文件
		-》外部表：
			-》删除时会删除元数据
			-》删除时不会删除HDFS的数据文件，保证了数据的安全性
			
-》企业中
	-》全部创建外部表，同时创建多张表，用于分析不同的业务
	-》通过location指定同一份数据源




		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
